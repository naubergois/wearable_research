# -*- coding: utf-8 -*-
"""WearableReserach.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10H0XpA3q4qm7wkI7t6iIyQ8KJzRRLDvr
"""

import os

# instalar kaggle
!pip install -q kaggle

# criar pasta
os.makedirs("/root/.kaggle", exist_ok=True)

# Fazer upload do seu kaggle.json
from google.colab import files
uploaded = files.upload()

# mover kaggle.json para a pasta certa
!mv kaggle.json /root/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

# baixar dataset WESAD
!kaggle datasets download -d orvile/wesad-wearable-stress-affect-detection-dataset

# descompactar
!unzip -q wesad-wearable-stress-affect-detection-dataset.zip -d wesad

!pip install -q tensorflow
!pip install -q numpy pandas scikit-learn

!find /content -maxdepth 5 -type f -name "*.pkl"

import os
import pickle
import numpy as np
import pandas as pd
from glob import glob

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix

import tensorflow as tf
from tensorflow.keras import layers, models

WESAD_ROOT = "/content/wesad/WESAD"

WINDOW_SEC = 16
TARGET_SAMPLING_HZ = 4

BINARY_LABELS = {1:0, 2:1, 3:0}

def load_subject_pkl(path):
    with open(path, "rb") as f:
        return pickle.load(f, encoding="latin1")

def resample_signal(signal, old_fs, new_fs):
    if old_fs == new_fs:
        return signal
    ratio = old_fs / new_fs
    idx = (np.arange(0, len(signal)) / ratio).astype(int)
    idx = idx[idx < len(signal)]
    return signal[idx]




def create_windows(X, y, window_sec=16, fs=4, stride_sec=4):
    """
    Cria janelas deslizantes com majority vote para stress vs não-stress.
    Agora com verificações de segurança e debug.
    """

    window_size = window_sec * fs
    stride = stride_sec * fs

    Xw, Yw = [], []

    for start in range(0, len(X) - window_size, stride):
        end = start + window_size

        win_x = X[start:end]
        win_y = y[start:end]

        # Filtrar labels válidos
        valid = win_y[np.isin(win_y, [1, 2, 3])]

        if len(valid) == 0:
            # print(f"Descartada (sem labels válidos) start={start}")
            continue

        if len(valid) < fs:  # menos de 1s de labels válidos
            # print(f"Descartada (poucos labels) start={start}")
            continue

        bin_labels = np.array([BINARY_LABELS[int(x)] for x in valid])
        label = int(bin_labels.mean() >= 0.5)

        Xw.append(win_x)
        Yw.append(label)

    return np.array(Xw), np.array(Yw)

def prepare_wrist_signals(data):
    """
    Extrai sinais do pulso (E4) e alinha labels usando mapeamento proporcional.
    Funciona para TODOS os participantes do WESAD.
    """

    wrist = data["signal"]["wrist"]

    # Sinais
    eda = wrist["EDA"].flatten()
    temp = wrist["TEMP"].flatten()
    bvp = wrist["BVP"].flatten()
    acc = wrist["ACC"]

    # Reamostrar para 4 Hz
    eda_rs = resample_signal(eda, 4, TARGET_SAMPLING_HZ)
    temp_rs = resample_signal(temp, 4, TARGET_SAMPLING_HZ)
    bvp_rs = resample_signal(bvp, 64, TARGET_SAMPLING_HZ)
    acc_x   = resample_signal(acc[:,0], 32, TARGET_SAMPLING_HZ)
    acc_y   = resample_signal(acc[:,1], 32, TARGET_SAMPLING_HZ)
    acc_z   = resample_signal(acc[:,2], 32, TARGET_SAMPLING_HZ)

    min_len = min(len(eda_rs), len(temp_rs), len(bvp_rs),
                  len(acc_x), len(acc_y), len(acc_z))

    X = np.stack([
        eda_rs[:min_len],
        temp_rs[:min_len],
        bvp_rs[:min_len],
        acc_x[:min_len],
        acc_y[:min_len],
        acc_z[:min_len]
    ], axis=-1)

    # -------- ALIGNMENT CORRECTED --------

    labels = data["label"].flatten()
    len_labels = len(labels)

    # Mapeamento proporcional direto (NUNCA use Hz aqui)
    idx_map = (np.linspace(0, len_labels-1, min_len)).astype(int)
    labels_aligned = labels[idx_map]

    return X, labels_aligned

def load_all_subjects(root=WESAD_ROOT):
    """
    Carrega todos os sujeitos do WESAD em qualquer estrutura de diretórios.
    Agora muito mais robusto.
    """

    # Busca recursiva — FUNCIONA para qualquer formato
    files = sorted(glob(os.path.join(root, "**", "*.pkl"), recursive=True))

    if len(files) == 0:
        raise FileNotFoundError(
            f"Nenhum arquivo .pkl encontrado em {root}.\n"
            "Verifique se o dataset foi extraído corretamente."
        )

    X_all, y_all = [], []

    for f in files:
        print("\n============================")
        print("Carregando participante:", f)

        try:
            data = load_subject_pkl(f)
        except:
            print("❌ Erro ao carregar:", f)
            continue

        X, y = prepare_wrist_signals(data)

        Xw, yw = create_windows(X, y)

        print(f"Janelas válidas: {len(Xw)}")

        if len(Xw) == 0:
            print("⚠ Nenhuma janela válida — ignorando sujeito")
            continue

        X_all.append(Xw)
        y_all.append(yw)

    if len(X_all) == 0:
        raise ValueError("Nenhum sujeito gerou janelas válidas.")

    X_all = np.vstack(X_all)
    y_all = np.concatenate(y_all)

    print("\nFINAL:")
    print("X_all:", X_all.shape)
    print("y_all:", y_all.shape)

    return X_all, y_all

def scale_features(X):
    n, T, F = X.shape
    reshaped = X.reshape(-1, F)
    scaler = StandardScaler()
    scaled = scaler.fit_transform(reshaped)
    return scaled.reshape(n, T, F), scaler

def build_lstm(input_shape):
    i = layers.Input(shape=input_shape)
    x = layers.LSTM(64, return_sequences=True)(i)
    x = layers.LSTM(32)(x)
    x = layers.Dropout(0.5)(x)
    x = layers.Dense(32, activation="relu")(x)
    o = layers.Dense(1, activation="sigmoid")(x)
    model = models.Model(i, o)
    model.compile(
        loss="binary_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-3),
        metrics=["accuracy"]
    )
    return model

@tf.function
def grad(model, inputs):
    with tf.GradientTape() as tape:
        tape.watch(inputs)
        pred = model(inputs)
    return tape.gradient(pred, inputs)

def integrated_gradients(model, baseline, sample, steps=50):
    alphas = tf.linspace(0.0, 1.0, steps)
    grads = []
    for a in alphas:
        x = baseline + a * (sample - baseline)
        g = grad(model, x)
        grads.append(g)
    avg = tf.reduce_mean(tf.stack(grads), axis=0)
    return (sample - baseline) * avg

X, y = load_all_subjects()

X, scaler = scale_features(X)

X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42
)

model = build_lstm(X_train.shape[1:])
model.summary()

hist = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=30,
    batch_size=64
)

y_pred = (model.predict(X_test).flatten() >= 0.5).astype(int)

print(classification_report(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))

idx = np.where((y_test==1) & (y_pred==1))[0][0]

sample = X_test[idx:idx+1]
baseline = np.zeros_like(sample)

attr = integrated_gradients(model, baseline, sample).numpy()[0]

feature_names = ["EDA","TEMP","BVP","ACC_X","ACC_Y","ACC_Z"]
importance = attr.mean(axis=0)

pd.DataFrame({
    "feature": feature_names,
    "importance_IG": importance
}).to_csv("wesad_feature_importance.csv", index=False)

importance

pd.DataFrame({
    "feature": feature_names,
    "importance_IG": importance
})

TARGET_WINDOW_SEC = 16         # tamanho da janela em segundos
EDA_FS = 4                     # EDA/TEMP (4 Hz)
ACC_FS = 32                    # ACC (32 Hz)
BVP_FS = 64                    # BVP (64 Hz)

BINARY_LABELS = {1:0, 2:1, 3:0}  # 1=baseline/0, 2=stress/1, 3=amusement/0

import numpy as np
from scipy.signal import find_peaks

def basic_stats(x):
    """Retorna [mean, std, min, max]"""
    return [
        float(np.mean(x)),
        float(np.std(x)),
        float(np.min(x)),
        float(np.max(x))
    ]

def slope_feature(x, fs):
    """Inclinação aproximada (tendência) em unidades por segundo."""
    if len(x) < 2:
        return 0.0
    return float((x[-1] - x[0]) / (len(x) / fs))

def peak_count(x, distance_samples=5):
    """Conta picos simples no sinal."""
    if len(x) == 0:
        return 0
    peaks, _ = find_peaks(x, distance=distance_samples)
    return int(len(peaks))

def bvp_hr_features(bvp_segment, fs=BVP_FS):
    """
    Calcula HR e HRV muito simples a partir do BVP em 64 Hz:
    - HR: batimentos por minuto
    - HRV: desvio padrão dos intervalos RR (em segundos)
    """
    if len(bvp_segment) < fs * 2:  # menos de 2s -> irrelevante
        return 0.0, 0.0

    # picos com distância mínima ~0.4s (freq cardíaca máxima ~150 bpm)
    min_distance = int(0.4 * fs)
    peaks, _ = find_peaks(bvp_segment, distance=min_distance)

    if len(peaks) < 2:
        return 0.0, 0.0

    duration_sec = len(bvp_segment) / fs
    hr_bpm = (len(peaks) / duration_sec) * 60.0

    rr_intervals = np.diff(peaks) / fs  # em segundos
    hrv_sd = float(np.std(rr_intervals)) if len(rr_intervals) > 0 else 0.0

    return float(hr_bpm), hrv_sd

def prepare_signals_and_labels(data):
    """
    Extrai sinais do pulso (Empatica E4) e alinha labels ao eixo do EDA (4 Hz)
    usando mapeamento proporcional por tamanho.

    Retorna:
      eda_4hz, temp_4hz, acc_32hz, bvp_64hz, labels_aligned_to_eda
    """
    wrist = data["signal"]["wrist"]

    eda = wrist["EDA"].flatten()    # 4 Hz
    temp = wrist["TEMP"].flatten()  # 4 Hz
    bvp = wrist["BVP"].flatten()    # 64 Hz
    acc = wrist["ACC"]              # [T_acc, 3] em 32 Hz

    labels = data["label"].flatten()  # ~700 Hz alinhado ao peito, mas sincronizado temporalmente

    len_eda = len(eda)
    len_temp = len(temp)
    len_acc = len(acc)
    len_bvp = len(bvp)
    len_labels = len(labels)

    # Mapear cada amostra do EDA para índices em TEMP, ACC, BVP e LABELS
    idx_eda_to_temp = (np.linspace(0, len_temp - 1, len_eda)).astype(int)
    idx_eda_to_acc  = (np.linspace(0, len_acc  - 1, len_eda)).astype(int)
    idx_eda_to_bvp  = (np.linspace(0, len_bvp  - 1, len_eda)).astype(int)
    idx_eda_to_lbl  = (np.linspace(0, len_labels - 1, len_eda)).astype(int)

    # Alinhar sinais na timeline do EDA
    temp_aligned   = temp[idx_eda_to_temp]
    acc_aligned    = acc[idx_eda_to_acc]   # ainda em 32 Hz, mas indexado por timeline do EDA (1:1)
    bvp_aligned    = bvp[idx_eda_to_bvp]   # BVP 64 Hz colapsado para "amostra por amostra de EDA"
    labels_aligned = labels[idx_eda_to_lbl]

    return eda, temp_aligned, acc_aligned, bvp, labels_aligned  # note: bvp original 64 Hz ainda será usado por janela

def extract_window_features_for_subject(eda, temp_aligned, acc_aligned, bvp_raw, labels_aligned,
                                        window_sec=TARGET_WINDOW_SEC, fs_eda=EDA_FS, stride_sec=4):
    """
    Cria janelas de 'window_sec' segundos na timeline do EDA (4 Hz),
    extrai features resumo de EDA, TEMP, ACC e BVP, e gera rótulos binários.

    eda:          1D (4 Hz)
    temp_aligned: 1D (mapeada para mesma timeline do EDA)
    acc_aligned:  2D [N_eda, 3]  (cada amostra -> índice proporcional de ACC)
    bvp_raw:      1D (64 Hz ORIGINAL)
    labels_aligned: 1D (mapeada para timeline do EDA)
    """
    window_size = window_sec * fs_eda
    stride = stride_sec * fs_eda

    X_feat_list = []
    y_list = []

    len_eda = len(eda)
    len_bvp = len(bvp_raw)

    # mapeamento global: índice do EDA para índice do BVP (64 Hz)
    idx_eda_to_bvp = (np.linspace(0, len_bvp - 1, len_eda)).astype(int)

    for start in range(0, len_eda - window_size, stride):
        end = start + window_size

        eda_win   = eda[start:end]
        temp_win  = temp_aligned[start:end]
        acc_win   = acc_aligned[start:end]  # shape [T_eda_win, 3]
        lbl_win   = labels_aligned[start:end]

        # labels válidos
        valid = lbl_win[np.isin(lbl_win, [1, 2, 3])]
        if len(valid) == 0:
            continue

        bin_labels = np.array([BINARY_LABELS[int(v)] for v in valid])
        label = int(bin_labels.mean() >= 0.5)

        # --- features EDA ---
        eda_stats = basic_stats(eda_win)
        eda_slope = slope_feature(eda_win, fs_eda)
        eda_peaks = peak_count(eda_win, distance_samples=2)

        # --- features TEMP ---
        temp_stats = basic_stats(temp_win)
        temp_slope = slope_feature(temp_win, fs_eda)

        # --- features ACC (magnitude) ---
        acc_mag = np.linalg.norm(acc_win, axis=1)
        acc_stats = basic_stats(acc_mag)
        acc_peaks = peak_count(acc_mag, distance_samples=2)

        # --- features BVP: pegar trecho correspondente em 64 Hz ---
        bvp_idx_start = idx_eda_to_bvp[start]
        bvp_idx_end   = idx_eda_to_bvp[end-1] if end-1 < len(idx_eda_to_bvp) else idx_eda_to_bvp[-1]
        if bvp_idx_end <= bvp_idx_start:
            bvp_idx_end = min(bvp_idx_start + int(window_sec * BVP_FS), len_bvp)

        bvp_seg = bvp_raw[bvp_idx_start:bvp_idx_end]

        bvp_stats = basic_stats(bvp_seg)
        hr, hrv = bvp_hr_features(bvp_seg, fs=BVP_FS)

        # concatenar todas as features em um vetor
        feats = (
            eda_stats + [eda_slope, eda_peaks] +
            temp_stats + [temp_slope] +
            acc_stats + [acc_peaks] +
            bvp_stats + [hr, hrv]
        )

        X_feat_list.append(feats)
        y_list.append(label)

    if len(X_feat_list) == 0:
        return np.empty((0, 0)), np.array([])

    X_feat = np.array(X_feat_list)
    y_arr  = np.array(y_list, dtype=int)

    return X_feat, y_arr

from glob import glob
import os

def load_all_subjects_features(root=WESAD_ROOT):
    """
    Percorre todos os S*.pkl, extrai features de janelas para cada sujeito
    e concatena em um único dataset tabular.
    """
    files = sorted(glob(os.path.join(root, "**", "S*.pkl"), recursive=True))

    if len(files) == 0:
        raise FileNotFoundError(f"Nenhum S*.pkl encontrado em {root}")

    X_all_list = []
    y_all_list = []

    for f in files:
        print("\n============================")
        print("Carregando participante:", f)

        data = load_subject_pkl(f)
        eda, temp_aligned, acc_aligned, bvp_raw, labels_aligned = prepare_signals_and_labels(data)

        X_feat, y = extract_window_features_for_subject(
            eda, temp_aligned, acc_aligned, bvp_raw, labels_aligned
        )

        print("Janelas válidas:", len(y))

        if X_feat.shape[0] == 0:
            print("⚠ Nenhuma janela válida — ignorando sujeito.")
            continue

        X_all_list.append(X_feat)
        y_all_list.append(y)

    if len(X_all_list) == 0:
        raise ValueError("Nenhum sujeito gerou janelas com features.")

    X_all = np.vstack(X_all_list)
    y_all = np.concatenate(y_all_list)

    print("\nFINAL:")
    print("X_all:", X_all.shape)
    print("y_all:", y_all.shape)

    return X_all, y_all

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow.keras import layers, models

def build_mlp(input_dim):
    i = layers.Input(shape=(input_dim,))
    x = layers.Dense(64, activation="relu")(i)
    x = layers.Dropout(0.4)(x)
    x = layers.Dense(32, activation="relu")(x)
    o = layers.Dense(1, activation="sigmoid")(x)
    model = models.Model(i, o)
    model.compile(
        loss="binary_crossentropy",
        optimizer=tf.keras.optimizers.Adam(1e-3),
        metrics=["accuracy"]
    )
    return model

# --- IG versão tabular ---

@tf.function
def grad_tabular(model, inputs):
    with tf.GradientTape() as tape:
        tape.watch(inputs)
        pred = model(inputs)
    return tape.gradient(pred, inputs)

def integrated_gradients_tabular(model, baseline, sample, steps=50):
    alphas = tf.linspace(0.0, 1.0, steps)
    grads = []
    for a in alphas:
        x = baseline + a * (sample - baseline)
        g = grad_tabular(model, x)
        grads.append(g)
    avg = tf.reduce_mean(tf.stack(grads), axis=0)
    return (sample - baseline) * avg

# 1. Carregar dataset de features
X, y = load_all_subjects_features(WESAD_ROOT)

# 2. Escalar
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 3. Split
X_train, X_temp, y_train, y_temp = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y
)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

# 4. Modelo
model = build_mlp(X_train.shape[1])
model.summary()

hist = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=30,
    batch_size=64,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(
            monitor="val_loss", patience=5, restore_best_weights=True
        )
    ]
)

# 5. Avaliação
y_pred_prob = model.predict(X_test).flatten()
y_pred = (y_pred_prob >= 0.5).astype(int)

print(classification_report(y_test, y_pred, target_names=["no_stress","stress"]))
print(confusion_matrix(y_test, y_pred))

# 6. IG em um exemplo de stress corretamente classificado
idx_stress = np.where((y_test == 1) & (y_pred == 1))[0]
if len(idx_stress) > 0:
    idx = idx_stress[0]
    sample = X_test[idx:idx+1]
    baseline = np.zeros_like(sample)

    attr = integrated_gradients_tabular(model,
                                        tf.cast(baseline, tf.float32),
                                        tf.cast(sample, tf.float32),
                                        steps=50).numpy()[0]

    # nomes das features na mesma ordem da extração
    feature_names = [
        "EDA_mean","EDA_std","EDA_min","EDA_max","EDA_slope","EDA_peaks",
        "TEMP_mean","TEMP_std","TEMP_min","TEMP_max","TEMP_slope",
        "ACC_mean","ACC_std","ACC_min","ACC_max","ACC_peaks",
        "BVP_mean","BVP_std","BVP_min","BVP_max","HR","HRV"
    ]

    df_attr = pd.DataFrame({"feature": feature_names,
                            "IG_importance": attr})
    print(df_attr.sort_values("IG_importance", ascending=False))
else:
    print("Nenhum exemplo de stress corretamente classificado para IG.")

